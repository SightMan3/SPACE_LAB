{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c45d58-7c21-4ec4-955f-3be1387943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82df663a-6371-4c4d-931d-4d91611eb1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda:0', True, '12.4', 1, 'NVIDIA GeForce GTX 1650 Ti')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device, torch.cuda.is_available(), torch.version.cuda, torch.cuda.device_count(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae05c392-b9f9-40ba-a3f8-58e6fb9d3dbb",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007a3dc-bbb8-43f0-851d-b79a8953ae0c",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3291ba-247a-4a4a-b4d8-d65910f1c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(a, b, c, d, y, n):\n",
    "    '''\n",
    "    Function creates batches out of a, b, c, d data of size 8 hours,\n",
    "    8-hour window is moved throughout the arrays with step one hour,\n",
    "    Than all the 8-hour batches are concatenated together to create one big 32 size batch.\n",
    "    Finally appended to main array.\n",
    "\n",
    "    Also cuts first 8 hours out of y data, so that for every 8 hours of x data\n",
    "    there is one hour of y data hour ahead of input neurons data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a - Bz\n",
    "    b - Sigma Bz\n",
    "    c - n\n",
    "    d = v\n",
    "    y - y training data (DST)\n",
    "    n - length of event\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batches - Matrix where rows correspond to 8 hours of x array data\n",
    "          y - y training data (DST) that has first 8 hours cut off\n",
    "    '''\n",
    "\n",
    "    y = y[8:]\n",
    "    y = np.array(y)\n",
    "\n",
    "    batches = []\n",
    "\n",
    "    for i in range(n):\n",
    "        if (i+8) <= n:\n",
    "            # print(i, i+8)\n",
    "            batch_a = a[i:i+8]\n",
    "            batch_b = b[i:i+8]\n",
    "            batch_c = c[i:i+8]\n",
    "            batch_d = d[i:i+8]\n",
    "\n",
    "            final_batch = np.concatenate((batch_a, batch_b, batch_c, batch_d), axis=None)\n",
    "\n",
    "            batches.append(final_batch)\n",
    "\n",
    "    batches = batches[:-1]\n",
    "    batches = np.array(batches)\n",
    "\n",
    "    return batches, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7809c6c4-9400-4022-b6da-774dcdea5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataProcessing():\n",
    "    '''\n",
    "    Function Loops through trainind data in steps of 146. Because every event is 146 hours long.\n",
    "    For every 146 hours of every parameter (Bz, sigma Bz, n, v, DST) is created a matrix with dimensions 146 X 32. of x data\n",
    "    and vector of  size 146 values of y data. One y value for 32 values of parameters.\n",
    "    This matrix is than added to final array. Which in the end corresponds to tensor of size 60 X 146 X 32 and matrix 60 X 146.\n",
    "    60 Because there are 60 events in training data. And we need to have 32 values for every DST values so there is 146 X 32 matrix\n",
    "    of parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train data\n",
    "    y_train data\n",
    "\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv('./data/train_dst_new.csv')\n",
    "\n",
    "    x_train_batches = []\n",
    "    y_train_batches = []\n",
    "\n",
    "    for i in range(0, len(data), 147):\n",
    "        Bz       = data.loc[i:i+146]['Bz_GSE'][:-1].to_numpy()\n",
    "        Bz_sigma = data.loc[i:i+146]['Sigma_Bz_GSE'][:-1].to_numpy()\n",
    "        n        = data.loc[i:i+146]['Proton_density'][:-1].to_numpy()\n",
    "        v        = data.loc[i:i+146]['Plasma_speed'][:-1].to_numpy()\n",
    "        DST      = data.loc[i:i+146]['Dst_index'][:-1].to_numpy()\n",
    "\n",
    "        x_train, y_train = generate_batches(Bz, Bz_sigma, n, v, DST, 146)\n",
    "\n",
    "        y_train_batches.append(y_train)\n",
    "        x_train_batches.append(x_train)\n",
    "\n",
    "    return (x_train_batches, y_train_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0d848-95bd-4c87-bc20-ffc619ab1342",
   "metadata": {},
   "source": [
    "x_train: There is 60 events, for event there is 138 arrays of 32 values. 32 values correspond to 8 hours of every parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b760fad-f66b-4b39-b0c8-6503e52f5052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 138, 32), (60, 138))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = DataProcessing()\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcabba-9339-4809-bb52-5bc78da34a20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Split events to matrix $\\mathbb{R}^{60\\times138\\times8\\times4}$\n",
    "The way data are passed into LSTM are different than normal neural network. We have 8 hours for every parameter for 138 hours concatenated together, which results in matrix $\\mathbb{R}^{138\\times32}$. However LSTM needs every feature separatly, matter of fact, it needs the 8 hours for the 4 features in columns in a matrix $\\mathbb{R}^{8\\times4}$ times 138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25cf55da-58b5-4def-94af-2cd59c32d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 138, 8, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tra = x_train.reshape(60, 138, 4, 8)\n",
    "x_tra = np.transpose(x_tra, (0, 1, 3, 2))\n",
    "x_tra = torch.from_numpy(x_tra)\n",
    "x_tra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b353b8c0-6cb9-40e2-86de-65a740870968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 138, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tra = y_train.reshape(60, -1, 1)\n",
    "y_tra = torch.from_numpy(y_tra)\n",
    "y_tra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9a50a-2990-4795-a5ee-d09f829e42c9",
   "metadata": {},
   "source": [
    "# Create LSTM model\n",
    "\n",
    "LSTM or Long-Short-Term-Memory is a type of neural network that is able to remember complex relations within data. It's a special type of recurrent neural network. However, basic RNNs are highly prone to fail due to the vanishing or exploding gradient, which the LSTM NN fixes to some extent. The main difference between RNN and LSTM is that RNN has normal basic structure just like FFNN just there are some conections from the output to some hidden layer or something like that, nevertheless you just have input layers, recurrent layers, hidden layers and input layer and that's it. LSTM is quite different. In LSTM you have this things called cells. Every LSTM cell is a little recurrent neural network on its own. These cells, besides having NNs in them, they also have gates. These gates could be interpreted as parameters that are trainable and decide which data should be forgotten and which should remebered. In other words, what should be passed further and which should be discarded when new data comes.\n",
    "\n",
    "We could also show the distinction between the RNN and LSTM mathemathically and also learn about their innerworkings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25cca06-2a84-4006-88ec-a9ac84b5deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size, hidden_size, num_stacked_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        out    = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b40c47bc-ba6c-4907-b9a6-36d88f498280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn): LSTM(4, 69, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=69, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_n  = 69\n",
    "stacked_n = 3\n",
    "\n",
    "model = LSTM(4, hidden_n, stacked_n)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7a66a-fdd4-4007-8d2a-658a5dc63e6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "787cc2d8-44be-4afc-a1b3-c982cfcb1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, x, y, loss_f, optimizerm, silent):\n",
    "    for i in range(epochs):\n",
    "        index = 0\n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if silent != True:\n",
    "            print(f'EPOCH : {i}')\n",
    "        \n",
    "        for batch_x, batch_y in zip(x, y):\n",
    "            index += 1\n",
    "            \n",
    "            batch_x = batch_x.to(torch.float32).to(device)\n",
    "            batch_y = batch_y.to(torch.float32).to(device)\n",
    "\n",
    "            output = model(batch_x)\n",
    "            loss = loss_f(output, batch_y)\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if silent != True:\n",
    "                if index % 60 == 0:\n",
    "                    avg_loss_across_batches = running_loss / 100\n",
    "                    print(f'LOSS : {avg_loss_across_batches}')\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    print(\"DONE Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd39083d-cc58-4e44-972a-bb2ee8382e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0\n",
      "LOSS : 0.007425057608634234\n",
      "EPOCH : 1\n",
      "LOSS : 0.006735195342916995\n",
      "EPOCH : 2\n",
      "LOSS : 0.005313104898668826\n",
      "EPOCH : 3\n",
      "LOSS : 0.0051571848907042295\n",
      "EPOCH : 4\n",
      "LOSS : 0.004895049746264704\n",
      "EPOCH : 5\n",
      "LOSS : 0.004638868366018869\n",
      "EPOCH : 6\n",
      "LOSS : 0.004471949061262422\n",
      "EPOCH : 7\n",
      "LOSS : 0.004382791297975927\n",
      "EPOCH : 8\n",
      "LOSS : 0.004326287002768368\n",
      "EPOCH : 9\n",
      "LOSS : 0.004280085172504187\n",
      "EPOCH : 10\n",
      "LOSS : 0.004234302748809569\n",
      "EPOCH : 11\n",
      "LOSS : 0.004182976743904874\n",
      "EPOCH : 12\n",
      "LOSS : 0.0041229681496042754\n",
      "EPOCH : 13\n",
      "LOSS : 0.004051450149854645\n",
      "EPOCH : 14\n",
      "LOSS : 0.003980912353727035\n",
      "EPOCH : 15\n",
      "LOSS : 0.00392408981861081\n",
      "EPOCH : 16\n",
      "LOSS : 0.003877423732774332\n",
      "EPOCH : 17\n",
      "LOSS : 0.0038388708350248634\n",
      "EPOCH : 18\n",
      "LOSS : 0.0038051761157112196\n",
      "EPOCH : 19\n",
      "LOSS : 0.0037751639616908504\n",
      "EPOCH : 20\n",
      "LOSS : 0.0037477149412734434\n",
      "EPOCH : 21\n",
      "LOSS : 0.0037223138508852573\n",
      "EPOCH : 22\n",
      "LOSS : 0.0036984588694758715\n",
      "EPOCH : 23\n",
      "LOSS : 0.0036757255991688\n",
      "EPOCH : 24\n",
      "LOSS : 0.003654122989973985\n",
      "EPOCH : 25\n",
      "LOSS : 0.0036330973677104338\n",
      "EPOCH : 26\n",
      "LOSS : 0.003612965812208131\n",
      "EPOCH : 27\n",
      "LOSS : 0.003594102487550117\n",
      "EPOCH : 28\n",
      "LOSS : 0.0035743793280562388\n",
      "EPOCH : 29\n",
      "LOSS : 0.0035495329747209326\n",
      "EPOCH : 30\n",
      "LOSS : 0.003530971868894994\n",
      "EPOCH : 31\n",
      "LOSS : 0.003512953004683368\n",
      "EPOCH : 32\n",
      "LOSS : 0.0034855354559840636\n",
      "EPOCH : 33\n",
      "LOSS : 0.0034537416300736367\n",
      "EPOCH : 34\n",
      "LOSS : 0.0034471088362624867\n",
      "EPOCH : 35\n",
      "LOSS : 0.003419843399897218\n",
      "EPOCH : 36\n",
      "LOSS : 0.0033942073833895846\n",
      "EPOCH : 37\n",
      "LOSS : 0.003369683868368156\n",
      "EPOCH : 38\n",
      "LOSS : 0.0033447590284049512\n",
      "EPOCH : 39\n",
      "LOSS : 0.003327404241426848\n",
      "EPOCH : 40\n",
      "LOSS : 0.0033055654348572716\n",
      "EPOCH : 41\n",
      "LOSS : 0.003287227986729704\n",
      "EPOCH : 42\n",
      "LOSS : 0.0032677518663695084\n",
      "EPOCH : 43\n",
      "LOSS : 0.003248505700030364\n",
      "EPOCH : 44\n",
      "LOSS : 0.0032297201117035003\n",
      "EPOCH : 45\n",
      "LOSS : 0.0032114024774637074\n",
      "EPOCH : 46\n",
      "LOSS : 0.0031933031301014125\n",
      "EPOCH : 47\n",
      "LOSS : 0.0031758685619570313\n",
      "EPOCH : 48\n",
      "LOSS : 0.0031555776507593693\n",
      "EPOCH : 49\n",
      "LOSS : 0.0031312057049944996\n",
      "EPOCH : 50\n",
      "LOSS : 0.0031016107473988085\n",
      "EPOCH : 51\n",
      "LOSS : 0.003076705677667633\n",
      "EPOCH : 52\n",
      "LOSS : 0.0030458701262250543\n",
      "EPOCH : 53\n",
      "LOSS : 0.0030330689903348682\n",
      "EPOCH : 54\n",
      "LOSS : 0.003016676577972248\n",
      "EPOCH : 55\n",
      "LOSS : 0.0030097992753144354\n",
      "EPOCH : 56\n",
      "LOSS : 0.003006355221150443\n",
      "EPOCH : 57\n",
      "LOSS : 0.003008355788188055\n",
      "EPOCH : 58\n",
      "LOSS : 0.002976006881799549\n",
      "EPOCH : 59\n",
      "LOSS : 0.0029729834094177933\n",
      "EPOCH : 60\n",
      "LOSS : 0.0029404473421163857\n",
      "EPOCH : 61\n",
      "LOSS : 0.002951671751216054\n",
      "EPOCH : 62\n",
      "LOSS : 0.002918438503984362\n",
      "EPOCH : 63\n",
      "LOSS : 0.0029259758314583452\n",
      "EPOCH : 64\n",
      "LOSS : 0.0029025449161417783\n",
      "EPOCH : 65\n",
      "LOSS : 0.0029140245576854797\n",
      "EPOCH : 66\n",
      "LOSS : 0.002885036471998319\n",
      "EPOCH : 67\n",
      "LOSS : 0.0028942911350168287\n",
      "EPOCH : 68\n",
      "LOSS : 0.0028689958399627357\n",
      "EPOCH : 69\n",
      "LOSS : 0.002871937257004902\n",
      "EPOCH : 70\n",
      "LOSS : 0.0028546554676722735\n",
      "EPOCH : 71\n",
      "LOSS : 0.0028543164080474526\n",
      "EPOCH : 72\n",
      "LOSS : 0.0028395486425142735\n",
      "EPOCH : 73\n",
      "LOSS : 0.002840172256110236\n",
      "EPOCH : 74\n",
      "LOSS : 0.002825865102931857\n",
      "DONE Training\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 75\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(num_epochs, x_tra, y_tra, loss_function, optimizer, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c084d51-06bc-4087-8bfc-049d876400c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Validation Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f93b75e-6ecf-451c-8ce4-5f3785b3a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValidationDataProcessing(valid_data):\n",
    "\n",
    "    year_1979 = valid_data[valid_data.index.year == 1979]\n",
    "    year_1980 = valid_data[valid_data.index.year == 1980]\n",
    "    year_1981 = valid_data[valid_data.index.year == 1981]\n",
    "\n",
    "    x_valid_79, y_valid_79 = generate_batches(\n",
    "        year_1979['Bz_GSE'],\n",
    "        year_1979['Sigma_Bz_GSE'],\n",
    "        year_1979['Proton_density'],\n",
    "        year_1979['Plasma_speed'],\n",
    "        year_1979['Dst_index'],\n",
    "        len(year_1979)\n",
    "    )\n",
    "\n",
    "    x_valid_80, y_valid_80 = generate_batches(\n",
    "        year_1980['Bz_GSE'],\n",
    "        year_1980['Sigma_Bz_GSE'],\n",
    "        year_1980['Proton_density'],\n",
    "        year_1980['Plasma_speed'],\n",
    "        year_1980['Dst_index'],\n",
    "        len(year_1980)\n",
    "    )\n",
    "\n",
    "    x_valid_81, y_valid_81 = generate_batches(\n",
    "        year_1981['Bz_GSE'],\n",
    "        year_1981['Sigma_Bz_GSE'],\n",
    "        year_1981['Proton_density'],\n",
    "        year_1981['Plasma_speed'],\n",
    "        year_1981['Dst_index'],\n",
    "        len(year_1981)\n",
    "    )\n",
    "\n",
    "    return x_valid_79, y_valid_79, x_valid_80, y_valid_80, x_valid_81, y_valid_81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d6a1a5c-6657-416f-94f4-d0473aefca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.read_csv('./data/test_dst_new.csv')\n",
    "valid_data.set_index('index', inplace=True)\n",
    "valid_data.index = pd.to_datetime(valid_data.index)\n",
    "\n",
    "x_valid_79, y_valid_79, x_valid_80, y_valid_80, x_valid_81, y_valid_81 = ValidationDataProcessing(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a16e7c14-d2bb-446e-a535-e34723068e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_79 = x_valid_79.reshape(x_valid_79.shape[0], 4, 8)\n",
    "x_79 = torch.transpose(torch.from_numpy(x_79), 2, 1)\n",
    "x_79 = x_79.to(torch.float32).to(device)\n",
    "\n",
    "x_80 = x_valid_80.reshape(x_valid_80.shape[0], 4, 8)\n",
    "x_80 = torch.transpose(torch.from_numpy(x_80), 2, 1)\n",
    "x_80 = x_80.to(torch.float32).to(device)\n",
    "\n",
    "x_81 = x_valid_81.reshape(x_valid_81.shape[0], 4, 8)\n",
    "x_81 = torch.transpose(torch.from_numpy(x_81), 2, 1)\n",
    "x_81 = x_81.to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea52e6-d2f8-4265-9f05-e5816bf8419b",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1037c027-7b4e-42b6-8384-b6e23ac9f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds_79 = model(x_79).cpu().detach()\n",
    "preds_80 = model(x_80).cpu().detach()\n",
    "preds_81 = model(x_81).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2230d5db-2f4b-412e-999f-532c5186e9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25d7ae86ce0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(y_valid_79, label='real')\n",
    "plt.plot(preds_79, label='prediction', c='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479caf6-c420-40e5-b4f8-8118100e7198",
   "metadata": {},
   "source": [
    "### Combine All Year, 79-80-81, of validation data and prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4429eaa-02b0-4499-b857-db0be4806c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = np.concatenate((preds_79, preds_80, preds_81))\n",
    "Predictions = pd.DataFrame(data={\n",
    "    'LSTM_DST+1': Predictions.flatten()\n",
    "})\n",
    "\n",
    "stripped_valid = pd.concat([\n",
    "    valid_data['Dst_index'][valid_data.index.year == 1979].iloc[8:],\n",
    "    valid_data['Dst_index'][valid_data.index.year == 1980].iloc[8:],\n",
    "    valid_data['Dst_index'][valid_data.index.year == 1981].iloc[8:]\n",
    "])\n",
    "Predictions.index = stripped_valid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "510efe76-3b1d-491c-97c8-8bc4c22183ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25de46d6aa0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(np.concatenate((y_valid_79, y_valid_80, y_valid_81)), label='real')\n",
    "plt.plot(Predictions[\"LSTM_DST+1\"].values)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3eab5ae9-a98c-4b7c-acab-a3561d68b740",
   "metadata": {},
   "outputs": [],
   "source": [
    " Predictions.to_csv('./data/DST_prediction_LSTM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddff866-92a7-4b2d-837a-7fa88f3e32a6",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bb6aab5-5347-4a14-a44e-a35b6f7db19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36ae0811-c600-4672-9dcf-385e2c9c5dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048028</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.060807</td>\n",
       "      <td>0.328861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042027</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.052090</td>\n",
       "      <td>0.360834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046877</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.061203</td>\n",
       "      <td>0.547739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE       MSE      RMSE       R^2\n",
       "0  0.048028  0.003698  0.060807  0.328861\n",
       "1  0.042027  0.002713  0.052090  0.360834\n",
       "2  0.046877  0.003746  0.061203  0.547739"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def metrics(results):\n",
    "    data = []\n",
    "    for res in results:\n",
    "        mae = mean_absolute_error(res[0], res[1])\n",
    "        mse = mean_squared_error(res[0], res[1])\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(res[0], res[1])\n",
    "        data.append([mae, mse, rmse, r2])\n",
    "    \n",
    "    return pd.DataFrame(data, columns=[\"MAE\", \"MSE\", \"RMSE\", \"R^2\"])\n",
    "    \n",
    "metrics(\n",
    "    [[y_valid_79, preds_79], \n",
    "     [y_valid_80, preds_80], \n",
    "     [y_valid_81, preds_81]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f7b6a542-b0f5-447b-b69c-0b530f4ecc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './data/LSTM_h128_SL3_lr001_ep75.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879729d6-22c9-460f-93cf-65d626c69241",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "371fe58b-e3bd-46e6-8496-722a10077f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.floor(8280/(6*(4+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "65179a8e-ad43-4cb8-b074-2599a472bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Stacked Number Of Layers 1===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.4858253  -1.30266096 -1.58573781] with hidden = 331, stacked_n = 1\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.08137745 -0.02267548 -0.24116823] with hidden = 276, stacked_n = 1\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.18459495 -2.07081033 -2.16394647] with hidden = 236, stacked_n = 1\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.88336329 -1.73329576 -1.91506521] with hidden = 207, stacked_n = 1\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.03496847 -0.08261447 -0.1202734 ] with hidden = 184, stacked_n = 1\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [ 0.00696039 -0.06052716 -0.05197656] with hidden = 165, stacked_n = 1\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.45036098 -2.38626956 -2.37770996] with hidden = 150, stacked_n = 1\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.03378513 -0.02627782 -0.16207591] with hidden = 138, stacked_n = 1\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0229066  -0.01873505 -0.14395333] with hidden = 127, stacked_n = 1\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.06298471 -0.28582998  0.00936999] with hidden = 118, stacked_n = 1\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.68920966 -1.52719009 -1.75689753] with hidden = 110, stacked_n = 1\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.57724064 -0.37419866 -0.7864624 ] with hidden = 103, stacked_n = 1\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.55903565 -3.61935997 -3.25925118] with hidden = 97, stacked_n = 1\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.26748332 -0.11428163 -0.46601037] with hidden = 92, stacked_n = 1\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 1\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.18087108 -0.05805534 -0.37272331] with hidden = 87, stacked_n = 1\n",
      "\n",
      "===Stacked Number Of Layers 2===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.58936391 -0.40022963 -0.79252289] with hidden = 331, stacked_n = 2\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.03999705 -0.21875148 -0.01298723] with hidden = 276, stacked_n = 2\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02240642 -0.16446928 -0.02628327] with hidden = 236, stacked_n = 2\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.6076274  -1.44828049 -1.68398294] with hidden = 207, stacked_n = 2\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.57467769 -1.40522425 -1.65905965] with hidden = 184, stacked_n = 2\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-4.24793642 -4.4262457  -3.78622336] with hidden = 165, stacked_n = 2\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.64645551 -0.45336903 -0.84787335] with hidden = 150, stacked_n = 2\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.17460098 -3.19816369 -2.94855715] with hidden = 138, stacked_n = 2\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00425259 -0.10782016 -0.03761873] with hidden = 127, stacked_n = 2\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.74878207 -3.84146247 -3.40225747] with hidden = 118, stacked_n = 2\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.87293842 -3.97939393 -3.49512891] with hidden = 110, stacked_n = 2\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.40428812 -0.93488318 -0.06219779] with hidden = 103, stacked_n = 2\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.87201694 -3.98997153 -3.49385441] with hidden = 97, stacked_n = 2\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.10812334 -0.38320187  0.0065392 ] with hidden = 92, stacked_n = 2\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 2\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.51268056 -2.43680601 -2.42790124] with hidden = 87, stacked_n = 2\n",
      "\n",
      "===Stacked Number Of Layers 3===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0548596  -0.00057319 -0.20683759] with hidden = 331, stacked_n = 3\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.13180042 -0.0288645  -0.31336106] with hidden = 276, stacked_n = 3\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.21692642 -0.08293018 -0.41438248] with hidden = 236, stacked_n = 3\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.39214948 -0.22120769 -0.60003846] with hidden = 207, stacked_n = 3\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02680921 -0.00085491 -0.15876637] with hidden = 184, stacked_n = 3\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.4138431  -0.24130492 -0.62204687] with hidden = 165, stacked_n = 3\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00956865 -0.02581331 -0.11228603] with hidden = 150, stacked_n = 3\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.32860597 -0.16849355 -0.53490873] with hidden = 138, stacked_n = 3\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [ 0.00267534 -0.04503377 -0.07432087] with hidden = 127, stacked_n = 3\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.02141372 -0.82600636 -1.18410449] with hidden = 118, stacked_n = 3\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.49008661 -0.30672539 -0.69678326] with hidden = 110, stacked_n = 3\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.07539107 -0.88134992 -1.23152238] with hidden = 103, stacked_n = 3\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.04579038 -3.04561316 -2.84854006] with hidden = 97, stacked_n = 3\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-4.51184632 -4.74188886 -3.98516933] with hidden = 92, stacked_n = 3\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 3\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.26889724 -3.30495822 -3.0208732 ] with hidden = 87, stacked_n = 3\n",
      "\n",
      "===Stacked Number Of Layers 4===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.41975848 -0.24566419 -0.62771749] with hidden = 331, stacked_n = 4\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.13745732 -0.94538692 -1.28526576] with hidden = 276, stacked_n = 4\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.10419403 -1.98802593 -2.0935454 ] with hidden = 236, stacked_n = 4\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-8.56429494e-02 -3.32109240e-01 -3.08112983e-05] with hidden = 207, stacked_n = 4\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0417218  -0.00036053 -0.18517277] with hidden = 184, stacked_n = 4\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.11502578 -0.02043153 -0.29213614] with hidden = 165, stacked_n = 4\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.03163601 -0.1976189  -0.01474169] with hidden = 150, stacked_n = 4\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00080489 -0.04829477 -0.07936213] with hidden = 138, stacked_n = 4\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.4053089  -0.23294682 -0.61327716] with hidden = 127, stacked_n = 4\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.24983679 -1.0629091  -1.3822923 ] with hidden = 118, stacked_n = 4\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.10583674 -1.98972977 -2.09511339] with hidden = 110, stacked_n = 4\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.9647489  -2.95507762 -2.78303421] with hidden = 103, stacked_n = 4\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.36957918 -0.87445991 -0.05390106] with hidden = 97, stacked_n = 4\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.53828357 -0.35191713 -0.74357419] with hidden = 92, stacked_n = 4\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 4\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.5593046  -1.39333744 -1.64407177] with hidden = 87, stacked_n = 4\n",
      "\n",
      "===Stacked Number Of Layers 5===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02251938 -0.00517714 -0.149194  ] with hidden = 331, stacked_n = 5\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0077029  -0.02040358 -0.11247783] with hidden = 276, stacked_n = 5\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.90555995 -2.88760926 -2.73598022] with hidden = 236, stacked_n = 5\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.93301684 -0.73587597 -1.10600554] with hidden = 207, stacked_n = 5\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.76962062 -0.57278451 -0.95913713] with hidden = 184, stacked_n = 5\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.56679666 -1.40043695 -1.65058063] with hidden = 165, stacked_n = 5\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.6720808  -1.51425335 -1.73845003] with hidden = 150, stacked_n = 5\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.28409141 -0.1329987  -0.48801723] with hidden = 138, stacked_n = 5\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.86184522 -0.66416668 -1.04247996] with hidden = 127, stacked_n = 5\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.3722314  -0.87912038 -0.05466043] with hidden = 118, stacked_n = 5\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.38918081 -3.43957674 -3.11657712] with hidden = 110, stacked_n = 5\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.01564886 -0.01019575 -0.13375237] with hidden = 103, stacked_n = 5\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.66866186 -0.4746975  -0.86633374] with hidden = 97, stacked_n = 5\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.83223323 -0.63470165 -1.01591315] with hidden = 92, stacked_n = 5\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 5\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02065978 -0.16487003 -0.02173811] with hidden = 87, stacked_n = 5\n",
      "\n",
      "===Stacked Number Of Layers 6===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.13577775 -0.03141329 -0.31854671] with hidden = 331, stacked_n = 6\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.60356185 -1.44012117 -1.68132232] with hidden = 276, stacked_n = 6\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02143264 -0.00574568 -0.14689268] with hidden = 236, stacked_n = 6\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00040931 -0.04848587 -0.07838206] with hidden = 207, stacked_n = 6\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.69949538 -0.50440835 -0.89490622] with hidden = 184, stacked_n = 6\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-8.78593254e-05 -6.54156091e-02 -6.46224752e-02] with hidden = 165, stacked_n = 6\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.6919744  -0.49714839 -0.8879633 ] with hidden = 150, stacked_n = 6\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.76339636 -1.53247293 -0.20136168] with hidden = 138, stacked_n = 6\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.17595851 -0.51903313 -0.00611939] with hidden = 127, stacked_n = 6\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.42087975 -0.24650366 -0.62882906] with hidden = 118, stacked_n = 6\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.41284024 -0.94974947 -0.06769046] with hidden = 110, stacked_n = 6\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.36726343 -0.20054706 -0.57477545] with hidden = 103, stacked_n = 6\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.71503054 -1.45413974 -0.18126432] with hidden = 97, stacked_n = 6\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-5.2745771  -5.63496765 -4.56669514] with hidden = 92, stacked_n = 6\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 6\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-4.10031782 -4.26111593 -3.66865802] with hidden = 87, stacked_n = 6\n",
      "\n",
      "===Stacked Number Of Layers 7===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-4.21299579e-02 -8.53937972e-05 -1.85993407e-01] with hidden = 331, stacked_n = 7\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00135766 -0.04019876 -0.08641813] with hidden = 276, stacked_n = 7\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-9.58367306e-02 -3.54320218e-01 -1.25245572e-04] with hidden = 236, stacked_n = 7\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.05675966 -0.26418225 -0.00483003] with hidden = 207, stacked_n = 7\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.93860167 -1.80526066 -1.95844001] with hidden = 184, stacked_n = 7\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.36428486 -1.1838716  -1.47986455] with hidden = 165, stacked_n = 7\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.52048869 -0.33560222 -0.72637132] with hidden = 150, stacked_n = 7\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.23641075 -2.13475536 -2.20092658] with hidden = 138, stacked_n = 7\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.02058195 -3.0184317  -2.82692412] with hidden = 127, stacked_n = 7\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.54528873 -1.37728341 -1.63258425] with hidden = 118, stacked_n = 7\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.97197707 -0.77539485 -1.14051514] with hidden = 110, stacked_n = 7\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.70697409 -3.80566961 -3.3641494 ] with hidden = 103, stacked_n = 7\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.02693337 -0.0030959  -0.15818051] with hidden = 97, stacked_n = 7\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.12451904 -0.41547104 -0.00057049] with hidden = 92, stacked_n = 7\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 7\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.32742298 -0.16752547 -0.53371207] with hidden = 87, stacked_n = 7\n",
      "\n",
      "===Stacked Number Of Layers 8===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.98571533 -0.78938815 -1.15264114] with hidden = 331, stacked_n = 8\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00157826 -0.08283373 -0.0535881 ] with hidden = 276, stacked_n = 8\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.98164957 -0.78524432 -1.14905434] with hidden = 236, stacked_n = 8\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.39652266 -0.22542621 -0.60442126] with hidden = 207, stacked_n = 8\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.75696908 -0.56038545 -0.94763046] with hidden = 184, stacked_n = 8\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0263639  -0.18279409 -0.01727857] with hidden = 165, stacked_n = 8\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.07067964 -0.29750495 -0.00221248] with hidden = 150, stacked_n = 8\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.08571978 -0.33190425 -0.00062869] with hidden = 138, stacked_n = 8\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.58182118 -0.39236159 -0.78495528] with hidden = 127, stacked_n = 8\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.02316864 -0.82766775 -1.18559794] with hidden = 118, stacked_n = 8\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.68373082 -3.77884871 -3.346092  ] with hidden = 110, stacked_n = 8\n",
      "\n",
      "training 16, hidden_n = 103, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-1.08107580e-01 -3.80866069e-01 -1.72665694e-05] with hidden = 103, stacked_n = 8\n",
      "\n",
      "training 17, hidden_n = 97, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00092439 -0.04323927 -0.08332649] with hidden = 97, stacked_n = 8\n",
      "\n",
      "training 18, hidden_n = 92, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-5.28390222 -5.64589751 -4.57378742] with hidden = 92, stacked_n = 8\n",
      "\n",
      "training 19, hidden_n = 87, stacked_n = 8\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-3.37818483 -3.42725507 -3.1078439 ] with hidden = 87, stacked_n = 8\n",
      "\n",
      "===Stacked Number Of Layers 9===\n",
      "\n",
      "training 5, hidden_n = 331, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00617279 -0.02342974 -0.10754921] with hidden = 331, stacked_n = 9\n",
      "\n",
      "training 6, hidden_n = 276, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.77933009 -0.58235477 -0.96799578] with hidden = 276, stacked_n = 9\n",
      "\n",
      "training 7, hidden_n = 236, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-2.3159839  -2.22344971 -2.26520895] with hidden = 236, stacked_n = 9\n",
      "\n",
      "training 8, hidden_n = 207, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.12098834 -0.02361902 -0.29981967] with hidden = 207, stacked_n = 9\n",
      "\n",
      "training 9, hidden_n = 184, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.22705211 -0.09015791 -0.42575178] with hidden = 184, stacked_n = 9\n",
      "\n",
      "training 10, hidden_n = 165, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.0273008  -0.18559258 -0.01666906] with hidden = 165, stacked_n = 9\n",
      "\n",
      "training 11, hidden_n = 150, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.81627613 -0.61887231 -1.00147439] with hidden = 150, stacked_n = 9\n",
      "\n",
      "training 12, hidden_n = 138, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.07864299 -0.00591382 -0.24264195] with hidden = 138, stacked_n = 9\n",
      "\n",
      "training 13, hidden_n = 127, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.00318593 -0.09416946 -0.04745361] with hidden = 127, stacked_n = 9\n",
      "\n",
      "training 14, hidden_n = 118, stacked_n = 9\n",
      "DONE Training\n",
      "R^2 values [79, 80, 81] = [-0.63244774 -1.31916864 -0.14802641] with hidden = 118, stacked_n = 9\n",
      "\n",
      "training 15, hidden_n = 110, stacked_n = 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, hidden_n = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, stacked_n = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstacked_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     21\u001b[0m preds_79 \u001b[38;5;241m=\u001b[39m model(x_79)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[102], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, x, y, loss_f, optimizerm, silent)\u001b[0m\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m silent \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\space_lab\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\space_lab\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\space_lab\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_r2s = []\n",
    "\n",
    "for s in range(1, 10):\n",
    "    print()\n",
    "    print(f\"===Stacked Number Of Layers {s}===\")\n",
    "\n",
    "    r2s = []\n",
    "    \n",
    "    for i in range(5, 20):\n",
    "        hidden_n  = math.floor(8280/(i*(4+1)))\n",
    "        stacked_n = s\n",
    "        \n",
    "        model = LSTM(4, hidden_n, stacked_n)\n",
    "        model.to(device)\n",
    "    \n",
    "        print()\n",
    "        print(f'training {i}, hidden_n = {hidden_n}, stacked_n = {stacked_n}')\n",
    "        train(num_epochs, x_tra, y_tra, loss_function, optimizer, silent=True)\n",
    "    \n",
    "        model.eval()\n",
    "        preds_79 = model(x_79).cpu().detach()\n",
    "        preds_80 = model(x_80).cpu().detach()\n",
    "        preds_81 = model(x_81).cpu().detach()\n",
    "    \n",
    "        metrics_r2s = metrics(\n",
    "            [[y_valid_79, preds_79], \n",
    "             [y_valid_80, preds_80], \n",
    "             [y_valid_81, preds_81]]\n",
    "        )['R^2'].values\n",
    "        r2s.append(metrics_r2s)\n",
    "        \n",
    "        print(\n",
    "            f\"R^2 values [79, 80, 81] = {metrics_r2s} with hidden = {hidden_n}, stacked_n = {stacked_n}\"\n",
    "        )\n",
    "    all_r2s.append(r2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28070faf-7597-4c08-b78d-93d9e13e53ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38145095-fb60-43b4-8383-0f642ccd703e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
